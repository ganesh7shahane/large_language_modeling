{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an LSTM on a single sentence\n",
    "\n",
    "In my early days of practising Pytorch and training LSTMs on text for next character prediction, I with struggled with some of the basics, often ending up with errors.\n",
    "\n",
    "But then, I managed to successfully implement one.\n",
    "\n",
    "This notebook is about training a very LSTM on a very simple training dataset consisting of a simple sequence \"I am passionate about computational drug discovery\", and the accompanying commentary.\n",
    "\n",
    "Let's get the dataset ready first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"i am passionate about computational drug discovery\"\n",
    "\n",
    "#Length of the sequence\n",
    "seq_len = len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's keep the sentence small, only considering all lowercase letters, a space and a terminator symbol (#, to mark end of sentence) as my allowed set of characters.\n",
    "\n",
    "This can be generated in python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "Letter set is abcdefghijklmnopqrstuvwxyz #\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "vocabulary = string.ascii_lowercase+' #'\n",
    "\n",
    "n_vocabulary = len(vocabulary)\n",
    "print(n_vocabulary)\n",
    "print('Letter set is '+vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can’t directly input characters to the LSTM model and hence we need to convert into numbers; some form of encoding to feed our model. \n",
    "\n",
    "The method of choice for most ML practisioners is usually one hot encoding. This encoding is quite simple and straight forward. There are 28 allowed letters. (26 letters + 1 space + 1 ‘#’). I will then convert each of my character into a 28 dimensional vector. All the other dimensions have value 0, except the index of character in our string letters, which we defined above.\n",
    "\n",
    "So, letter ```a``` will be encoded into ```[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]```. Similarly ```b``` will be encoded as ```[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]``` and so on...\n",
    "\n",
    "```#``` is encoded as ```[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]```. \n",
    "\n",
    "We will be dealing with these vectors from now on. In pytorch, basic datatype is tensor, which is a multidimensional array. So, we store all our encodings in tensors.\n",
    "\n",
    "Below function converts a letter to tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding of 'a'  tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Encoding of 'b'  tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Encoding of '#'  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "#This function takes a character and returns an encoded vector\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def ltt(ch):\n",
    "    ans = torch.zeros(n_vocabulary)\n",
    "    ans[vocabulary.find(ch)]=1\n",
    "    return ans\n",
    "\n",
    "print(\"Encoding of 'a' \",ltt('a'))\n",
    "print(\"Encoding of 'b' \",ltt('b'))\n",
    "print(\"Encoding of '#' \",ltt('#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function takes a string and returns a 3D tensor\n",
    "\n",
    "def getLine(s):\n",
    "    ans = []\n",
    "    for c in s:\n",
    "        ans.append(ltt(c))\n",
    "    return torch.cat(ans,dim=0).view(len(s),1,n_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s define our Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our neural network class. every Neural Network in pytorch extends nn.Module\n",
    "\n",
    "class MyLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,hidden_dim):\n",
    "        super(MyLSTM,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #LSTM takes, input dimensions, hidden dimensions and num layers in case of stacked LSTMs (Default is 1)\n",
    "        self.LSTM = nn.LSTM(input_dim,hidden_dim)\n",
    "        \n",
    "#Input must be 3 dimensional (Sequence len, batch, input dimensions)\n",
    "#hc is a tuple which contains the vectors h (hidden/feedback) and c (cell state vector)\n",
    "\n",
    "    def forward(self,inp,hc):\n",
    "        #this gives outut for each input and also (hidden and cell state vector)\n",
    "        output,_= self.LSTM(inp,hc)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s use the above class to initialize our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensions of output of neural network is (seq_len, batch , hidden_dim). Since we want output dimensions to be\n",
    "#the same as n_vocabulary, hidden_dim = n_vocabulary (output dimensions = hidden_dimensions)\n",
    "hidden_dim = n_vocabulary     \n",
    "\n",
    "#Invoking model. Input dimensions = n_vocabulary i.e 28. output dimensions = hidden_dimensions = 28\n",
    "model = MyLSTM(n_vocabulary,hidden_dim)\n",
    "\n",
    "#I'm using Adam optimizer here\n",
    "optimizer = torch.optim.Adam(params = model.parameters(),lr=0.01)\n",
    "\n",
    "#Loss function is CrossEntropyLoss\n",
    "LOSS = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we are trying to build is a character generation model. That means, we will input the sequence , “i am passionate about computational drug discovery” to the model. Since this is a supervised technique, we must have a dataset to train the model. I’m taking the output for a sequence is, next letter in the sequence. \n",
    "\n",
    "That means, for the input sequence “i am passio”, output character must be “n”, since n is the next letter in the original sequence. \n",
    "\n",
    "Similarily, for input sequence “i am pa”, output sequence is “s”. \n",
    "\n",
    "Now, let’s try to build actual outputs. Let’s call them ‘targets’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([26,  0, 12, 26, 15,  0, 18, 18,  8, 14, 13,  0, 19,  4, 26,  0,  1, 14,\n",
       "        20, 19, 26,  2, 14, 12, 15, 20, 19,  0, 19,  8, 14, 13,  0, 11, 26,  3,\n",
       "        17, 20,  6, 26,  3,  8, 18,  2, 14, 21,  4, 17, 24, 27])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List to store targets\n",
    "targets = []\n",
    "\n",
    "#Iterate through all chars in the sequence, starting from second letter. Since output for 1st letter is the 2nd letter\n",
    "for x in data[1:]+'#':\n",
    "    #Find the target index. For a, it is 0, For 'b' it is 1 etc..\n",
    "    targets.append(vocabulary.find(x))\n",
    "\n",
    "#Convert into tensor\n",
    "targets = torch.tensor(targets)\n",
    "\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s build input sequence to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List to store input\n",
    "inpl = []\n",
    "\n",
    "#Iterate through all inputs in the sequence\n",
    "for c in data:\n",
    "    #Convert into tensor\n",
    "    inpl.append(ltt(c))\n",
    "\n",
    "#Convert list to tensor\n",
    "inp = torch.cat(inpl, dim=0)\n",
    "\n",
    "#Reshape tensor into 3 dimensions (sequence length, batches = 1, dimensions = n_vocabulary (28))\n",
    "inp = inp.view(seq_len, 1, n_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the training begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   tensor(3.3191, grad_fn=<NllLossBackward0>)\n",
      "10   tensor(3.0017, grad_fn=<NllLossBackward0>)\n",
      "20   tensor(2.6948, grad_fn=<NllLossBackward0>)\n",
      "30   tensor(2.4703, grad_fn=<NllLossBackward0>)\n",
      "40   tensor(2.3628, grad_fn=<NllLossBackward0>)\n",
      "50   tensor(2.2722, grad_fn=<NllLossBackward0>)\n",
      "60   tensor(2.2023, grad_fn=<NllLossBackward0>)\n",
      "70   tensor(2.1528, grad_fn=<NllLossBackward0>)\n",
      "80   tensor(2.1192, grad_fn=<NllLossBackward0>)\n",
      "90   tensor(2.0845, grad_fn=<NllLossBackward0>)\n",
      "100   tensor(2.0473, grad_fn=<NllLossBackward0>)\n",
      "110   tensor(2.0167, grad_fn=<NllLossBackward0>)\n",
      "120   tensor(2.1217, grad_fn=<NllLossBackward0>)\n",
      "130   tensor(2.0452, grad_fn=<NllLossBackward0>)\n",
      "140   tensor(2.0015, grad_fn=<NllLossBackward0>)\n",
      "0.39878296852111816\n"
     ]
    }
   ],
   "source": [
    "#Let's note down start time to track the training time\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "#Number of iterations\n",
    "n_iters = 150\n",
    "\n",
    "for itr in range(n_iters):\n",
    "    \n",
    "    #Zero the previosus gradients\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #Initialize h and c vectors\n",
    "    h = torch.rand(hidden_dim).view(1,1,hidden_dim)\n",
    "    c = torch.rand(hidden_dim).view(1,1,hidden_dim)\n",
    "    \n",
    "    #Find the output\n",
    "    output = model(inp,(h,c))\n",
    "    \n",
    "    #Reshape the output to 2 dimensions. This is done, so that we can compare with target and get loss\n",
    "    output = output.view(seq_len,n_vocabulary)\n",
    "    \n",
    "    #Find loss\n",
    "    loss = LOSS(output,targets)\n",
    "    \n",
    "    #Print loss for every 10th iteration\n",
    "    if itr%10==0:\n",
    "        print(itr,' ',(loss) )\n",
    "        \n",
    "    #Back propagate the loss\n",
    "    loss.backward()\n",
    "    \n",
    "    #Perform weight updation\n",
    "    optimizer.step()\n",
    "    \n",
    "print((time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function that predicts next letter , given the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This utility method predicts the next letter given the sequence  \n",
    "\n",
    "def predict(s):\n",
    "    #Get the vector for input\n",
    "    inp = getLine(s)\n",
    "    \n",
    "    #Initialize h and c vectors\n",
    "    h = torch.rand(1,1,hidden_dim)\n",
    "    c = torch.rand(1,1,hidden_dim)\n",
    "    \n",
    "    #Get the output\n",
    "    out = model(inp,(h,c))\n",
    "    \n",
    "    #Find the corresponding letter from the output\n",
    "    return vocabulary[out[-1][0].topk(1)[1].detach().numpy().item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('i am passionate abou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I provide input ‘i love neu’, it’s able to correctly predict the next letter as ‘r’. Except for the highlighted sequence, it’s able to correctly predict for the other inputs. :-)\n",
    "\n",
    "The following function that recursively generates the sequence given an input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THis method recursively generates the sequence using the trained model\n",
    "\n",
    "def gen(s):\n",
    "    #If generated sequence length is too large, or terminate char is generated, we can print the generated output so far\n",
    "    if s[-1]=='#' or len(s)>=len(data)+5:\n",
    "        print(s)\n",
    "        return\n",
    "\n",
    "    #Predict with sequence s\n",
    "    pred = predict(s)\n",
    "    \n",
    "    #Continue prediction with sequence s + predicted value\n",
    "    print(s+pred)\n",
    "    \n",
    "    #Recurse\n",
    "    gen(s+pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am pas\n",
      "i am pass\n",
      "i am passi\n",
      "i am passio\n",
      "i am passion\n",
      "i am passiona\n",
      "i am passionaa\n",
      "i am passionaaa\n",
      "i am passionaaat\n",
      "i am passionaaat \n",
      "i am passionaaat b\n",
      "i am passionaaat bo\n",
      "i am passionaaat bou\n",
      "i am passionaaat bout\n",
      "i am passionaaat bout \n",
      "i am passionaaat bout c\n",
      "i am passionaaat bout co\n",
      "i am passionaaat bout com\n",
      "i am passionaaat bout comp\n",
      "i am passionaaat bout compu\n",
      "i am passionaaat bout comput\n",
      "i am passionaaat bout computa\n",
      "i am passionaaat bout computat\n",
      "i am passionaaat bout computati\n",
      "i am passionaaat bout computatio\n",
      "i am passionaaat bout computation\n",
      "i am passionaaat bout computationa\n",
      "i am passionaaat bout computational\n",
      "i am passionaaat bout computational \n",
      "i am passionaaat bout computational d\n",
      "i am passionaaat bout computational dr\n",
      "i am passionaaat bout computational dru\n",
      "i am passionaaat bout computational drud\n",
      "i am passionaaat bout computational drud \n",
      "i am passionaaat bout computational drud i\n",
      "i am passionaaat bout computational drud io\n",
      "i am passionaaat bout computational drud io \n",
      "i am passionaaat bout computational drud io c\n",
      "i am passionaaat bout computational drud io co\n",
      "i am passionaaat bout computational drud io coo\n",
      "i am passionaaat bout computational drud io cooe\n",
      "i am passionaaat bout computational drud io cooer\n",
      "i am passionaaat bout computational drud io cooery\n",
      "i am passionaaat bout computational drud io cooery#\n",
      "i am passionaaat bout computational drud io cooery#\n"
     ]
    }
   ],
   "source": [
    "gen(\"i am pa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the model has been overfitted by training on multiple iterations using the same SMILES again and again. \n",
    "\n",
    "Important points to remember :\n",
    "\n",
    "* Using Adam optimizer , loss comes down quickly compared to using SGD.\n",
    "\n",
    "* I’ve clearly witnessed the problem with vanilla RNN implementation. i.e vanishing gradient problem. The model weights quickly turned to NaNs that would stop my model from being trained further.\n",
    "\n",
    "* Incorrect dimensions are a problem while implementing. Special care needs to be taken to ensure that all the inputs dimensions are correct.\n",
    "\n",
    "* Don’t forget to zero the existing gradient values by using optimizer.zero_grad(). If we fail to do that, gradient values get accumulated and model won’t get trained correctly, hence will never converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkit310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
